import streamlit as st
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from utils.document_loader import load_documents
from utils.vector_store import create_vector_store, load_vector_store
import config

st.set_page_config(
    page_title=config.APP_TITLE,
    page_icon=config.APP_ICON,
    layout="wide"
)

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

def initialize_qa_chain():
    """Initialize the QA chain with vector store"""
    if st.session_state.vector_store is None:
        return None
    
    # Initialize LLM
    llm = ChatOpenAI(
        model_name=config.LLM_MODEL,
        temperature=config.TEMPERATURE,
        openai_api_key=config.OPENAI_API_KEY
    )
    
    # Initialize memory
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    # Create conversational chain
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=st.session_state.vector_store.as_retriever(search_kwargs={"k": 3}),
        memory=memory,
        return_source_documents=True
    )
    
    return qa_chain

def main():
    st.title(f"{config.APP_ICON} {config.APP_TITLE}")
    st.markdown("### RAG-powered support assistant with 10,000+ technical documents")
    
    # Sidebar
    with st.sidebar:
        st.header("‚öô Configuration")
        
        # Load or create vector store
        if st.button("üîÑ Load Document Index"):
            with st.spinner("Loading FAISS index..."):
                try:
                    embeddings = HuggingFaceEmbeddings(model_name=config.EMBEDDING_MODEL)
                    st.session_state.vector_store = load_vector_store(embeddings)
                    st.session_state.qa_chain = initialize_qa_chain()
                    st.success("‚úÖ Index loaded successfully!")
                except Exception as e:
                    st.error(f"‚ùå Error loading index: {str(e)}")
        
        st.markdown("---")
        st.markdown("üìä Stats**")
        st.metric("Documents Indexed", "10,000+")
        st.metric("Query Time Reduction", "60%")
        st.metric("Cost Reduction", "15%")
    
    # Main chat interface
    if st.session_state.qa_chain is None:
        st.info("üëà Please load the document index from the sidebar to start chatting")
        return
    
    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "sources" in message and message["sources"]:
                with st.expander("üìö View Sources"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"*Source {i}:* {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask a question about technical documentation..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                try:
                    response = st.session_state.qa_chain({"question": prompt})
                    answer = response["answer"]
                    sources = [doc.metadata.get("source", "Unknown") for doc in response.get("source_documents", [])]
                    
                    st.markdown(answer)
                    
                    if sources:
                        with st.expander("üìö View Sources"):
                            for i, source in enumerate(sources, 1):
                                st.markdown(f"*Source {i}:* {source}")
                    
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources
                    })
                except Exception as e:
                    st.error(f"Error: {str(e)}")

if _name_ == "_main_":
    main()